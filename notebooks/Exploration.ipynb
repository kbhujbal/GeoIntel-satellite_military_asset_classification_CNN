{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeoIntel - MVRSD Dataset Exploration\n",
    "\n",
    "## Military Vehicle Remote Sensing Dataset Analysis\n",
    "\n",
    "This notebook provides comprehensive exploration of the MVRSD dataset for military vehicle detection in satellite imagery.\n",
    "\n",
    "### Contents:\n",
    "1. Dataset Loading & Verification\n",
    "2. Ground Truth Visualization\n",
    "3. Class Distribution Analysis\n",
    "4. Object Size Analysis (Critical for Small Object Detection)\n",
    "5. Tiling Strategy Visualization\n",
    "6. Sample Inference Comparison (SAHI vs Standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import yaml\n",
    "\n",
    "# GeoIntel modules\n",
    "from src.data_loader import MVRSDDataLoader\n",
    "from src.tiling_utils import ImageTiler, visualize_tiling_grid\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [14, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Loading & Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and verify dataset structure\n",
    "DATA_DIR = '../data/raw'\n",
    "CONFIG_PATH = '../config/config.yaml'\n",
    "\n",
    "loader = MVRSDDataLoader(DATA_DIR, CONFIG_PATH)\n",
    "is_valid = loader.verify_structure()\n",
    "\n",
    "if is_valid:\n",
    "    print(\"\\n✅ Dataset is ready for exploration!\")\n",
    "else:\n",
    "    print(\"\\n❌ Please ensure dataset is properly structured in data/raw/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset statistics\n",
    "if is_valid:\n",
    "    stats = loader.get_statistics()\n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"  Total Images: {stats.total_images}\")\n",
    "    print(f\"  Total Annotations: {stats.total_annotations}\")\n",
    "    print(f\"  Average Objects/Image: {stats.avg_objects_per_image}\")\n",
    "    print(f\"\\nClass Distribution:\")\n",
    "    for cls, count in stats.class_distribution.items():\n",
    "        print(f\"    {cls}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ground Truth Visualization\n",
    "\n",
    "Visualize ground truth bounding boxes on satellite images to understand the annotation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class colors for visualization\n",
    "CLASS_COLORS = {\n",
    "    0: 'red',       # tank\n",
    "    1: 'green',     # truck\n",
    "    2: 'blue',      # cargo\n",
    "    3: 'yellow'     # military_vehicle\n",
    "}\n",
    "\n",
    "CLASS_NAMES = ['tank', 'truck', 'cargo', 'military_vehicle']\n",
    "\n",
    "def visualize_ground_truth(image_path, label_path, ax=None):\n",
    "    \"\"\"\n",
    "    Visualize ground truth bounding boxes on an image.\n",
    "    \n",
    "    YOLO format: class_id x_center y_center width height (normalized)\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    img = cv2.imread(str(image_path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "    \n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'{Path(image_path).name} ({w}x{h})', fontsize=12)\n",
    "    \n",
    "    # Load and draw annotations\n",
    "    if Path(label_path).exists():\n",
    "        with open(label_path, 'r') as f:\n",
    "            annotations = f.readlines()\n",
    "        \n",
    "        for ann in annotations:\n",
    "            parts = ann.strip().split()\n",
    "            if len(parts) != 5:\n",
    "                continue\n",
    "                \n",
    "            class_id = int(parts[0])\n",
    "            x_center, y_center, box_w, box_h = map(float, parts[1:])\n",
    "            \n",
    "            # Convert from normalized to pixel coordinates\n",
    "            x_min = (x_center - box_w/2) * w\n",
    "            y_min = (y_center - box_h/2) * h\n",
    "            box_w_px = box_w * w\n",
    "            box_h_px = box_h * h\n",
    "            \n",
    "            # Draw rectangle\n",
    "            color = CLASS_COLORS.get(class_id, 'white')\n",
    "            rect = patches.Rectangle(\n",
    "                (x_min, y_min), box_w_px, box_h_px,\n",
    "                linewidth=2, edgecolor=color, facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Add label\n",
    "            label = CLASS_NAMES[class_id] if class_id < len(CLASS_NAMES) else f'class_{class_id}'\n",
    "            ax.text(x_min, y_min - 5, label, color=color, fontsize=8, fontweight='bold')\n",
    "    \n",
    "    ax.axis('off')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images with ground truth\n",
    "if is_valid:\n",
    "    images_dir = Path(DATA_DIR) / 'images'\n",
    "    labels_dir = Path(DATA_DIR) / 'labels'\n",
    "    \n",
    "    # Get first few images\n",
    "    image_files = sorted(images_dir.glob('*.jpg'))[:6]\n",
    "    if not image_files:\n",
    "        image_files = sorted(images_dir.glob('*.png'))[:6]\n",
    "    \n",
    "    if image_files:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for ax, img_path in zip(axes, image_files):\n",
    "            label_path = labels_dir / f\"{img_path.stem}.txt\"\n",
    "            visualize_ground_truth(img_path, label_path, ax)\n",
    "        \n",
    "        plt.suptitle('Ground Truth Annotations (MVRSD Dataset)', fontsize=14, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No images found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_class_distribution(labels_dir):\n",
    "    \"\"\"Analyze class distribution across the dataset.\"\"\"\n",
    "    class_counts = Counter()\n",
    "    objects_per_image = []\n",
    "    \n",
    "    labels_dir = Path(labels_dir)\n",
    "    for label_file in labels_dir.glob('*.txt'):\n",
    "        with open(label_file, 'r') as f:\n",
    "            lines = [l.strip() for l in f.readlines() if l.strip()]\n",
    "        \n",
    "        objects_per_image.append(len(lines))\n",
    "        \n",
    "        for line in lines:\n",
    "            parts = line.split()\n",
    "            if parts:\n",
    "                class_id = int(parts[0])\n",
    "                class_counts[class_id] += 1\n",
    "    \n",
    "    return class_counts, objects_per_image\n",
    "\n",
    "if is_valid:\n",
    "    class_counts, objects_per_image = analyze_class_distribution(Path(DATA_DIR) / 'labels')\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Class distribution bar chart\n",
    "    classes = [CLASS_NAMES[i] if i < len(CLASS_NAMES) else f'class_{i}' for i in sorted(class_counts.keys())]\n",
    "    counts = [class_counts[i] for i in sorted(class_counts.keys())]\n",
    "    colors = [CLASS_COLORS.get(i, 'white') for i in sorted(class_counts.keys())]\n",
    "    \n",
    "    axes[0].bar(classes, counts, color=colors, edgecolor='white')\n",
    "    axes[0].set_xlabel('Class')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Class Distribution')\n",
    "    for i, (cls, count) in enumerate(zip(classes, counts)):\n",
    "        axes[0].text(i, count + max(counts)*0.02, str(count), ha='center', fontsize=10)\n",
    "    \n",
    "    # Objects per image histogram\n",
    "    axes[1].hist(objects_per_image, bins=20, color='cyan', edgecolor='white', alpha=0.7)\n",
    "    axes[1].set_xlabel('Objects per Image')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title(f'Objects per Image Distribution (mean: {np.mean(objects_per_image):.1f})')\n",
    "    axes[1].axvline(np.mean(objects_per_image), color='red', linestyle='--', label='Mean')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Object Size Analysis - THE SMALL OBJECT PROBLEM\n",
    "\n",
    "This is **CRITICAL** for understanding why SAHI is necessary. We analyze:\n",
    "- Object sizes relative to image dimensions\n",
    "- What happens when we resize to standard YOLO input (640x640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_object_sizes(images_dir, labels_dir):\n",
    "    \"\"\"Analyze object sizes in pixels and relative to image dimensions.\"\"\"\n",
    "    sizes_pixels = []  # (width, height) in pixels\n",
    "    sizes_relative = []  # (width, height) relative to image\n",
    "    image_dimensions = []\n",
    "    \n",
    "    images_dir = Path(images_dir)\n",
    "    labels_dir = Path(labels_dir)\n",
    "    \n",
    "    for img_path in list(images_dir.glob('*.jpg'))[:100] + list(images_dir.glob('*.png'))[:100]:\n",
    "        label_path = labels_dir / f\"{img_path.stem}.txt\"\n",
    "        if not label_path.exists():\n",
    "            continue\n",
    "            \n",
    "        # Get image dimensions\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            continue\n",
    "        h, w = img.shape[:2]\n",
    "        image_dimensions.append((w, h))\n",
    "        \n",
    "        # Parse annotations\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    continue\n",
    "                _, _, _, box_w, box_h = map(float, parts)\n",
    "                \n",
    "                # Pixel dimensions\n",
    "                w_px = box_w * w\n",
    "                h_px = box_h * h\n",
    "                sizes_pixels.append((w_px, h_px))\n",
    "                sizes_relative.append((box_w * 100, box_h * 100))  # Percentage\n",
    "    \n",
    "    return sizes_pixels, sizes_relative, image_dimensions\n",
    "\n",
    "if is_valid:\n",
    "    sizes_pixels, sizes_relative, image_dims = analyze_object_sizes(\n",
    "        Path(DATA_DIR) / 'images', \n",
    "        Path(DATA_DIR) / 'labels'\n",
    "    )\n",
    "    \n",
    "    if sizes_pixels:\n",
    "        sizes_pixels = np.array(sizes_pixels)\n",
    "        sizes_relative = np.array(sizes_relative)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Object sizes in pixels\n",
    "        axes[0, 0].scatter(sizes_pixels[:, 0], sizes_pixels[:, 1], alpha=0.5, s=10, c='cyan')\n",
    "        axes[0, 0].set_xlabel('Width (pixels)')\n",
    "        axes[0, 0].set_ylabel('Height (pixels)')\n",
    "        axes[0, 0].set_title('Object Sizes (Pixels)')\n",
    "        axes[0, 0].axhline(20, color='red', linestyle='--', alpha=0.7, label='20px threshold')\n",
    "        axes[0, 0].axvline(20, color='red', linestyle='--', alpha=0.7)\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        # Histogram of object widths\n",
    "        axes[0, 1].hist(sizes_pixels[:, 0], bins=50, color='cyan', edgecolor='white', alpha=0.7)\n",
    "        axes[0, 1].set_xlabel('Object Width (pixels)')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].set_title(f'Object Width Distribution (median: {np.median(sizes_pixels[:, 0]):.1f}px)')\n",
    "        axes[0, 1].axvline(np.median(sizes_pixels[:, 0]), color='red', linestyle='--', label='Median')\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # Relative sizes\n",
    "        axes[1, 0].scatter(sizes_relative[:, 0], sizes_relative[:, 1], alpha=0.5, s=10, c='yellow')\n",
    "        axes[1, 0].set_xlabel('Width (% of image)')\n",
    "        axes[1, 0].set_ylabel('Height (% of image)')\n",
    "        axes[1, 0].set_title('Object Sizes (Relative to Image)')\n",
    "        \n",
    "        # What happens after resize to 640x640\n",
    "        if image_dims:\n",
    "            avg_img_width = np.mean([d[0] for d in image_dims])\n",
    "            resize_factor = 640 / avg_img_width\n",
    "            resized_sizes = sizes_pixels * resize_factor\n",
    "            \n",
    "            axes[1, 1].hist(resized_sizes[:, 0], bins=50, color='red', edgecolor='white', alpha=0.7)\n",
    "            axes[1, 1].set_xlabel('Object Width After 640px Resize')\n",
    "            axes[1, 1].set_ylabel('Frequency')\n",
    "            axes[1, 1].set_title(f'⚠️ PROBLEM: After Resize to 640px (median: {np.median(resized_sizes[:, 0]):.1f}px)')\n",
    "            axes[1, 1].axvline(5, color='yellow', linestyle='--', label='5px - Nearly invisible!')\n",
    "            axes[1, 1].legend()\n",
    "        \n",
    "        plt.suptitle('THE SMALL OBJECT PROBLEM - Why SAHI is Essential', fontsize=14, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SMALL OBJECT ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Average object size: {np.mean(sizes_pixels[:, 0]):.1f}x{np.mean(sizes_pixels[:, 1]):.1f} px\")\n",
    "        print(f\"Median object size: {np.median(sizes_pixels[:, 0]):.1f}x{np.median(sizes_pixels[:, 1]):.1f} px\")\n",
    "        print(f\"Objects < 20px: {np.sum(sizes_pixels[:, 0] < 20)} ({100*np.mean(sizes_pixels[:, 0] < 20):.1f}%)\")\n",
    "        print(f\"\\nAfter resize to 640px:\")\n",
    "        print(f\"  Median object becomes: {np.median(resized_sizes[:, 0]):.1f}x{np.median(resized_sizes[:, 1]):.1f} px\")\n",
    "        print(f\"  Objects < 5px: {np.sum(resized_sizes[:, 0] < 5)} ({100*np.mean(resized_sizes[:, 0] < 5):.1f}%)\")\n",
    "        print(\"\\n⚠️  This is why standard YOLO FAILS on satellite imagery!\")\n",
    "        print(\"✅  SAHI processes tiles at full resolution, preserving small objects.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tiling Strategy Visualization\n",
    "\n",
    "Visualize how SAHI slices images into overlapping tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tiling_strategy(image_path, tile_sizes=[256, 512, 640], overlap=0.2):\n",
    "    \"\"\"Visualize different tiling strategies.\"\"\"\n",
    "    img = cv2.imread(str(image_path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(tile_sizes) + 1, figsize=(18, 5))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(f'Original ({w}x{h})')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Different tile sizes\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    for ax, tile_size, color in zip(axes[1:], tile_sizes, colors):\n",
    "        ax.imshow(img, alpha=0.7)\n",
    "        \n",
    "        tiler = ImageTiler(tile_size=tile_size, overlap_ratio=overlap)\n",
    "        n_cols, n_rows, positions = tiler.calculate_tile_grid(w, h)\n",
    "        \n",
    "        for x_offset, y_offset in positions:\n",
    "            rect = patches.Rectangle(\n",
    "                (x_offset, y_offset), tile_size, tile_size,\n",
    "                linewidth=1, edgecolor=color, facecolor='none', alpha=0.8\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "        \n",
    "        ax.set_title(f'{tile_size}px tiles\\n{n_cols}x{n_rows}={len(positions)} tiles')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Tiling Strategies (Overlap: {int(overlap*100)}%)', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if is_valid:\n",
    "    images_dir = Path(DATA_DIR) / 'images'\n",
    "    sample_images = list(images_dir.glob('*.jpg'))[:1] or list(images_dir.glob('*.png'))[:1]\n",
    "    \n",
    "    if sample_images:\n",
    "        visualize_tiling_strategy(sample_images[0])\n",
    "    else:\n",
    "        print(\"No images found for tiling visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SAHI Inference Demo (Requires Model)\n",
    "\n",
    "Compare standard YOLO inference vs SAHI sliced inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell demonstrates SAHI inference\n",
    "# Requires: pip install sahi ultralytics\n",
    "\n",
    "DEMO_MODEL_PATH = '../models/geointel_best.pt'  # Update with your model path\n",
    "\n",
    "def run_sahi_comparison(image_path, model_path):\n",
    "    \"\"\"Compare SAHI vs standard inference on a single image.\"\"\"\n",
    "    try:\n",
    "        from src.geointel_eye import GeoIntelEye\n",
    "        \n",
    "        eye = GeoIntelEye(model_path=model_path)\n",
    "        \n",
    "        # SAHI inference\n",
    "        sahi_result = eye.scan(\n",
    "            image_path=str(image_path),\n",
    "            output_path=None,\n",
    "            visualize=False,\n",
    "            export_geojson=False\n",
    "        )\n",
    "        \n",
    "        # Standard inference\n",
    "        standard_result = eye.scan_standard(str(image_path))\n",
    "        \n",
    "        return sahi_result, standard_result\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"Missing dependency: {e}\")\n",
    "        print(\"Install with: pip install sahi ultralytics\")\n",
    "        return None, None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Model not found at {model_path}\")\n",
    "        print(\"Train a model first or download pretrained weights\")\n",
    "        return None, None\n",
    "\n",
    "# Uncomment to run inference comparison\n",
    "# if is_valid and Path(DEMO_MODEL_PATH).exists():\n",
    "#     sample_image = list((Path(DATA_DIR) / 'images').glob('*.jpg'))[0]\n",
    "#     sahi_result, std_result = run_sahi_comparison(sample_image, DEMO_MODEL_PATH)\n",
    "#     \n",
    "#     if sahi_result and std_result:\n",
    "#         print(f\"\\nSAHI detections: {sahi_result.total_detections}\")\n",
    "#         print(f\"Standard detections: {std_result.total_detections}\")\n",
    "#         improvement = sahi_result.total_detections - std_result.total_detections\n",
    "#         print(f\"Improvement: +{improvement} detections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **The Small Object Problem**: Military vehicles in satellite imagery are often 20x20 pixels or smaller\n",
    "\n",
    "2. **Why Standard YOLO Fails**: Resizing 4000x4000 images to 640x640 reduces objects to ~3-5 pixels - impossible to detect\n",
    "\n",
    "3. **SAHI Solution**: Process overlapping 512x512 tiles, maintaining full resolution for small objects\n",
    "\n",
    "### Recommended Configuration:\n",
    "- Tile size: 512x512\n",
    "- Overlap: 20%\n",
    "- Model: YOLOv8-Medium\n",
    "- Post-processing: NMS with IOU threshold 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
